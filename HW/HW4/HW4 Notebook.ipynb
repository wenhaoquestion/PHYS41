{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Attention is All You Need\n",
    "\n",
    "**Objective:**\n",
    "In this assignment, you will build a complete Self-Attention block for a single sentence from scratch. Here you will start with the **raw input vectors** and generate the Queries, Keys, and Values yourself using **Matrix Multiplication**. This mimics the actual architecture of a Transformer layer (like in GPT or BERT).\n",
    "\n",
    "### **Context: What is \"Word Embedding\"?**\n",
    "Before we do math, let's talk about data. Computers cannot understand strings like \"Apple\". To process language, we convert every word into a list of numbers called a **Vector**.\n",
    "* This conversion is called **Word Embedding**.\n",
    "* The key idea is that words with similar meanings (like \"King\" and \"Queen\") will have vectors that are numerically close to each other.\n",
    "* For this homework, we assume our sentence has **128 words**, and each word is represented by a vector of **size 16**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1: Setup and Weight Initialization**\n",
    "**Context:**\n",
    "In a real Transformer, we don't just use the input vectors directly. We \"project\" them into three different spaces: Query, Key, and Value. We do this by multiplying the input by three learnable weight matrices: $W_Q, W_K, W_V$.\n",
    "\n",
    "**Task:**\n",
    "1.  Set the random seed to `42`.\n",
    "2.  Generate a \"Input Sentence\" matrix $X$ of shape `(128, 16)` using standard random normal matrix (`rand`).\n",
    "    * 128 = Sequence Length (Words)\n",
    "    * 16 = Embedding Dimension ($d_{model}$)\n",
    "3.  Generate three **Weight Matrices** ($W_Q, W_K, W_V$).\n",
    "    * In this simple example, we will project from dimension 16 back to dimension 16.\n",
    "    * Shape of each weight matrix: `(16, 16)`.\n",
    "    * You can create these matrices using standard random normal matrix (`randn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Set seed\n",
    "\n",
    "# TODO: Generate Input X (128, 16)\n",
    "\n",
    "# TODO: Generate Weights W_q, W_k, W_v (16, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2: Linear Projections (Creating Q, K, V)**\n",
    "**Context:**\n",
    "Now we calculate the Query, Key, and Value matrices using the linear definitions:\n",
    "$$Q = X \\cdot W_Q$$\n",
    "$$K = X \\cdot W_K$$\n",
    "$$V = X \\cdot W_V$$\n",
    "\n",
    "**Task:**\n",
    "1.  Perform the matrix multiplication. You can use the `@` operator (standard for matrix multiplication) or `np.dot` or `np.einsum`\n",
    "2.  Verify that the resulting shapes of $Q, K, V$ are all `(128, 16)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Q, K, V using matrix multiplication\n",
    "\n",
    "# TODO: Print shapes to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3: The Attention Scores**\n",
    "**Context:**\n",
    "The core of attention is finding out how much every word \"cares\" about every other word. We do this by calculating the dot product between Queries and Keys.\n",
    "$$\\text{Scores} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "**Task:**\n",
    "1.  Multiply $Q$ by the **transpose** of $K$.\n",
    "    * $Q$ shape: `(128, 16)`\n",
    "    * $K^T$ shape: `(16, 128)`\n",
    "    * Result shape should be `(128, 128)` (A relationship map between every word and every other word).\n",
    "2.  Scale the result by dividing by $\\sqrt{16}$ (the dimension of the keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Scores\n",
    "# Hint: Use Q @ K.T\n",
    "\n",
    "# TODO: Scale the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4: The Softmax (Probability Distribution)**\n",
    "**Context:**\n",
    "The scores can be any number (negative, positive, huge). We want to turn them into probabilities that sum to 1.0 for each word.\n",
    "\n",
    "The formula for Softmax is:\n",
    "$$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $$\n",
    "\n",
    "**Task:**\n",
    "1.  Exponentiate the scores using `np.exp()`.\n",
    "2.  Divide each row by the sum of that row.\n",
    "    * **Crucial:** Ensure you are summing along `axis=1` (the rows) and use `keepdims=True` to allow for correct division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # TODO: Implement simple softmax (exp / sum)\n",
    "    pass\n",
    "\n",
    "# TODO: Apply to scores matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 5: The Final Representation**\n",
    "**Context:**\n",
    "Finally, we create the new representation for each word. This is the weighted sum of the Values ($V$), weighted by the attention probabilities.\n",
    "$$\\text{Output} = \\text{AttentionWeights} \\cdot V$$\n",
    "\n",
    "**Task:**\n",
    "1.  Multiply your `weights` matrix `(128, 128)` by the `V` matrix `(128, 16)`.\n",
    "2.  Print the final shape. It should be `(128, 16)`â€”the same as your input $X$, but now every word contains context from the whole sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
